
# Intelligent Chunking Strategies for OpenAPI with Complex Schema References in RAG

## Challenges with Naïve Chunking of OpenAPI Specs

Splitting a large OpenAPI specification into generic fixed-size or per-section chunks often fails for deeply nested schemas and references. Key problems include:

* **Schema Fragmentation:** Important type definitions (in `components/schemas`) get isolated from the endpoints that use them. For example, if the “Create Campaign” endpoint’s request schema is defined separately, a naive chunk-by-path approach will put the endpoint in one chunk and the `CreateCampaignRequest` schema in another. Critical details then end up in different chunks with no link between them. This makes it hard to retrieve all necessary info with a single query.

* **Broken `$ref` Pointers:** OpenAPI `$ref` references (e.g. `"$ref": "#/components/schemas/Campaign"`) lose meaning when the target schema is in a different chunk. A semantic search might return the endpoint chunk, but that chunk only contains a pointer string instead of the actual schema content. The LLM can’t interpret the pointer without the referenced chunk. As an example, an endpoint might reference schema A, which references B, which references C – if each is chunked separately, a user query like “What format does the create campaign API expect?” would only retrieve the endpoint definition (with `$ref` to A) but miss schemas B and C entirely.

* **Context Disconnection:** Parameters and examples lose context if separated from their parent endpoint. For instance, error codes or field descriptions defined in components may not be retrieved alongside the endpoint description, unless the chunking strategy preserves those relationships.

These issues mean that **traditional chunking (e.g. one chunk per path or fixed token windows)** can lead to incomplete results. In one simple approach, developers just “split the JSON into parts for each path” of the spec. While straightforward, this approach often fails to bring along the schemas and definitions that the endpoints reference.

## Schema Dependency Resolution (Resolving `$ref` Chains)

To enable accurate retrieval, **schema references must be resolved or linked** so that each query can access the full chain of definitions it needs. There are two general approaches:

* **Inline Schema Expansion:** Merge or inline referenced schemas into the same chunk as their parent endpoint (at least to some depth). This means when chunking the spec, if an endpoint uses `CreateCampaignRequest` which in turn uses `Campaign` → `TargetingCriteria`, you include those schema definitions in the endpoint’s chunk text. By resolving the `$ref` pointers, the chunk becomes self-contained and answerable. This was shown to be crucial in a tool-building context: Pranav Dhoolia notes that an OpenAPI-based assistant needed to **“make sure that the tool definition has resolved schema references”** because leaving `$ref`s meant the schema details were not available to the system. In our RAG setting, inlining schemas ensures a single retrieved chunk can answer questions that span endpoint and object definitions.

* **On-Demand Schema Linking:** An alternative is to keep endpoint and schema chunks separate but store explicit links/metadata so that when one is retrieved, the system can fetch the others. For example, each endpoint chunk’s metadata could list the component schemas it references (by name or ID), and each schema chunk could list which endpoints or other schemas reference it. The retrieval pipeline can then **automatically pull in related chunks**. For instance, if a user query hits the "Create Campaign" endpoint chunk, the system sees it references `Campaign` and `TargetingCriteria` schemas and then **adds those schema chunks** to the results. This yields a complete context without fully duplicating content in every chunk.

In practice, a **hybrid** of these approaches works best. You might inline one or two levels of references directly, and link deeper levels. For example, include the immediate schema (`CreateCampaignRequest` and its key fields) in the endpoint chunk, but if that schema itself has a nested `$ref` to a very large object, you could leave that for a separate chunk. The goal is to **cover typical question depth**: if most queries won’t go more than 2 levels deep into the schema, inline those two levels. Beyond that, maintain references via metadata or a second-stage retrieval. This balances completeness with chunk size.

Crucially, whichever approach you choose, **make sure the content of `$ref` schemas is accessible to the model**. If not in the same chunk, then via the retrieval process. Without that, users will hit dead-ends where the answer lies in an un-fetched schema.

## Optimal Chunk Granularity and Size

Choosing how much content to put in each chunk is a balancing act:

* **Chunks Too Large (Monolithic):** If you stuff too much into one chunk (e.g. an entire 2MB spec in one embedding), you’ll have very high recall but very low precision. The model will often retrieve that giant chunk for any query (since it contains almost everything), but most of the content will be irrelevant noise for a specific question. A recent study found that using **one chunk per full spec** gave recall near 90%, but precision under 10% – essentially, the chunk matches many queries but doesn’t specifically answer them well. This leads to poor relevance ranking and can confuse the LLM with extraneous data.

* **Chunks Too Small (Over-split):** If you split into tiny pieces (e.g. every object or paragraph separately), you preserve precision (each chunk is very focused) but risk missing important context. The user might get only a fragment of what they need. For example, splitting every schema as an isolated chunk means a query about an endpoint might retrieve just the endpoint description without the schema details (or vice versa), if the semantic search doesn’t connect them. Over-splitting also means the system has to assemble many pieces, increasing the chance of missing one. In the OpenAPI chunking study, splitting by JSON fields achieved very high recall (up to \~97%) but extremely poor precision (\~5%) because the search would pull in many small pieces that only partially matched the query.

* **Mixed/Logical Splitting:** The sweet spot is to split by **logical units** – e.g. one chunk per endpoint (including its params and maybe immediate schemas) and separate chunks for standalone schemas or groups of related schemas. Endpoints and schemas might even use different chunk sizes: endpoint chunks could be larger since they may include example requests/responses and some schema details, whereas pure schema-definition chunks can be a bit smaller and focused. This “mix of granularity” ensures each chunk is meaningful on its own. Research has indeed shown that **format-specific chunking (respecting the JSON structure of the API) outperforms naïve equal-sized chunks**. In other words, chunking along the lines of the API’s inherent structure (endpoints, objects, etc.) yields better retrieval accuracy.

To implement optimal granularity, consider the following strategy:

* **Endpoint Chunks:** Each chunk covers one endpoint’s documentation – including its method, path, summary, description, parameters, and **inlined schema references (to a certain depth)**. If the endpoint has a request or response body schema, pull in the key fields of those schemas here. This makes the chunk self-contained for questions like “How do I create a new campaign?” (it would contain the endpoint usage and the required fields). If an endpoint section is very large (approaching the 8K token limit due to many nested fields or lengthy descriptions), you might split that one endpoint into two chunks (e.g. “Create Campaign – Part 1” and “Part 2”) at a logical break (perhaps split the schema definition across parts). If you do split an endpoint into multiple chunks, **include some overlapping context or a clear note** so that retrieval of one part can lead to the other (for example, duplicate a sentence like “(continued)” with the operationId in both parts, or use metadata to mark them as parts of the same endpoint).

* **Schema Chunks:** Each top-level schema in `components/schemas` can be a chunk (or grouped with closely related schemas). Include the schema’s properties, required fields, and descriptions. Prepend some context like “**Schema:** Campaign (used in CreateCampaignRequest)” to each schema chunk’s content so that it’s searchable both by schema name and by related endpoint if possible. This way, a query like “What fields are in Campaign schema?” will directly retrieve the `Campaign` schema chunk with all its fields. Meanwhile, a query about the endpoint might retrieve the endpoint chunk (which mentions “Campaign” in it) *and* potentially the schema chunk if the vector similarity deems it relevant. By having both chunk types, we ensure both endpoint-centric and data-model-centric queries are covered.

* **Other Chunks (Errors, Common Info):** If the spec has other sections like common error codes, authentication info, etc., consider chunking those separately. For example, a chunk that lists all error codes and their meanings for a service, or one that describes authorization requirements. These provide context for queries like “What does error code 400 mean for this API?” which might not be answered in a single endpoint description. By pulling them out, you avoid repeating the same error definitions in every endpoint chunk. Instead, one **“Error Codes” chunk** can be retrieved alongside the relevant endpoint. (You might also duplicate *just the relevant error* in each endpoint chunk’s text if different endpoints have distinct error definitions, but if they share the same definitions, linking to a common chunk avoids redundancy.)

**Chunk Size:** Aim to keep chunks well under the embedding model’s 8K token limit – ideally a few hundred tokens each, up to maybe 1000-2000 tokens, for efficiency. The 8K is a hard ceiling, not a target; smaller chunks generally mean faster embedding and more precise semantic matches. Use 8K only as an upper bound for very large endpoint definitions that cannot be split without losing context. If a schema or endpoint is enormous (say 10,000 tokens), break it up logically (e.g. group properties into multiple chunks) and use overlap or metadata to indicate they belong together. Also remove any truly extraneous text: for instance, large example payloads can be omitted or summarized to save space (this was found to improve retrieval – an “Endpoint without examples” chunking had better precision in tests).

Finally, **preserve hierarchical context** in each chunk’s content. Include headings or labels like the API name, the service or tag, and the endpoint path at the top of each chunk. For example, an endpoint chunk might start with: “**AdCatalog API – Campaign Service – POST /campaigns**: Create Campaign – Description...”. A schema chunk might start with “**Schema:** TargetingCriteria (component schema in Campaign) – Fields: ...”. This ensures the model sees some context of where this piece fits (helping both the semantic search and the LLM’s understanding when formulating answers).

## Metadata for Cross-Reference and Context

Augmenting each chunk with rich **metadata** is highly recommended to maintain linkages:

* **Reference Lists:** For an endpoint chunk, have metadata listing all `$ref` schemas included or related. E.g. `"refs": ["CreateCampaignRequest", "Campaign", "TargetingCriteria"]`. For a schema chunk, include a reverse link like `"used_in": ["CreateCampaignRequest", "UpdateCampaignRequest", "CampaignService"]` (any operation or service that uses it). This metadata won’t be seen by the LLM, but your retrieval logic can use it.

* **Tags or Group IDs:** Group chunks by service or tag. For instance, all chunks relating to the “Campaigns” feature could have a tag “campaigns”. Using a vector database that supports filtering or grouping (like Qdrant or Pinecone), you can then optionally scope searches to a tag if the user query clearly pertains to one API subset. Nick Khami’s approach with Qdrant’s grouping is an example – he assigns chunks to groups based on their tags (collections of endpoints) so that search results can be organized by those groups. In practice, this means you could retrieve results grouped by API section, improving result diversity and clarity (“these two chunks are from Campaigns API, these two from AdGroups API”, etc., if a query spans multiple services).

* **Operation IDs and Summaries:** Include an operationId or a short summary in metadata. For example: `"operationId": "createCampaign", "summary": "Create a new advertising campaign."` This can help in debugging and also allow deterministic retrieval if needed (for instance, if the user query directly uses an operation name or if you want to implement a direct lookup by operationId).

* **Versioning and Source:** If you have multiple specs or versions, include metadata about which spec file or version the chunk came from (e.g. `"spec": "AdCatalog_v1.json"`). This helps with incremental updates (you know which spec to reprocess when something changes) and can be used to filter results to the relevant version if necessary.

Metadata doesn’t directly answer the question, but it’s the glue that **maintains integrity between chunks**. With the above in place, your system can perform smart retrieval: e.g., find the endpoint chunk via vector search, then automatically gather any related schema chunks by their names/IDs from metadata. This maintains the **“cross-reference integrity”** (Functional Req. #3) – the user gets the endpoint plus all connected pieces as one coherent answer, even though internally they were separate chunks.

## Retrieval Strategies to Include Related Chunks

Even with intelligent chunking, you’ll want to augment the retrieval step to ensure nothing is missed:

* **Multi-Stage Retrieval:** One design is a two-stage RAG retrieval. First, do a semantic search on the user query to get the top-k most relevant chunks. Then, **post-process those results**: for each retrieved chunk, check its metadata for any linked references (schemas or related parts). Pull in those linked chunks (perhaps with a lower rank or appended after the initial hits). This way, if an endpoint chunk is retrieved, you automatically attach the definitions it references. The user or the LLM will then see a complete picture. For example, a query “What format does the create campaign API expect?” might retrieve the “CreateCampaign (POST /campaigns)” endpoint chunk, and the system sees in metadata that it has `CreateCampaignRequest`. It then fetches the `CreateCampaignRequest` schema chunk (and that in turn might list `Campaign` schema, so grab that too). The final context fed to the LLM includes all pieces in the chain. This addresses the scenario where the query itself didn’t explicitly mention the schema names – the system proactively retrieves them.

* **Parallel Indexes or Hybrid Search:** You may maintain separate indices or namespaces for endpoints vs schemas, with different embedding strategies if needed. For instance, an “endpoint index” and a “schema index.” If a query looks like a pure data model question (“fields in UpdateAttributesRequestContent”), you might primarily query the schema index. If it looks like an operational question (“How do I create a campaign?”), query the endpoint index (which already has some schema info inline) and then follow up with pulling schema details as above. In many cases, though, a single unified index with good chunk design works, but this is an extra safeguard. Some systems even do keyword matching on schema names in addition to vector search, to ensure if user directly names a schema, that chunk is retrieved with high priority.

* **Semantic Cueing in Content:** Another subtle trick – since we want related chunks to surface together, ensure that the text of chunks provides some common ground for the vector model. For example, if an endpoint chunk says “requestBody: **CreateCampaignRequest** (object with fields: campaign, startDate, etc.)”, and the schema chunk for CreateCampaignRequest naturally contains the word “CreateCampaignRequest” (as its title or first line), then the embedding space will likely position those chunks closer when queries involve those terms. This means even the semantic search alone *could* retrieve both. The key is that we do **not** leave just a raw `$ref` with no description – at minimum, write the schema name in the endpoint chunk and perhaps a one-line summary of it. This improves the chance that a query which hints at data fields might still get a hit on the endpoint chunk. In short, use human-readable reference text in chunks (e.g. “campaign: *(Campaign object, see schema)*”) rather than solely machine-oriented `$ref` notation.

* **Ranking and Cut-off:** Because we are adding more chunks per query (via linked retrieval), we should be mindful of the LLM’s context limit when assembling the final prompt. Typically, you might limit to top 5 or so chunks. If each endpoint query is likely to bring in 2-3 chunks (the endpoint + 1-2 schemas), setting `k=5` or `k=6` could suffice. Alternatively, use a similarity threshold: retrieve all chunks above a certain cosine similarity score. This was mentioned as an open question – what is the optimal cut-off?. A safe approach is fixed top-k plus heuristic: always include directly referenced schema chunks even if their raw similarity was slightly lower, because you *know* they are relevant by construction.

Overall, the retrieval process should compensate for any remaining separation in your chunks. Think of it as **reassembling the document on the fly**: chunking may split things to store efficiently, but retrieval knits them back together for the question at hand.

## Balancing Completeness with Context Window Limits

We need to ensure that we provide as **complete an answer as possible (high recall)** without overwhelming the model with unnecessary text (maintaining precision and staying within token limits):

* **Completeness vs. Redundancy:** The target is that >95% of user queries retrieve all necessary info (Functional Req. #1 and Quality “Completeness”). Achieving this often means erring on the side of including something rather than leaving it out. For instance, it’s better to have a bit of overlapping information between an endpoint and a schema chunk than to risk a missing piece. Redundant embedding of a schema in two chunks is usually acceptable if it dramatically improves the chance of retrieval. Modern vector DBs can handle the storage, and you can deduplicate at answer time if needed. That said, to keep *embedding* load reasonable, try not to duplicate huge blocks of text everywhere. A good compromise is to **duplicate just the critical parts** (like field names and required fields) of a schema into endpoint chunks, but not necessarily the entire verbose description. The full description remains in the schema’s own chunk. This way, the endpoint chunk answers common queries concisely, and if the user needs more detail on a field, the schema chunk is also available.

* **Pruning and Summarization:** Leverage the fact that not all parts of the spec are equally important for retrieval. **Examples** and lengthy descriptive texts can be summarized or omitted in the chunk content to save space. The research by Pesl et al. implemented a “Remove examples” refinement which improved precision without hurting recall much. They also tried an LLM-based summarization of endpoints, which yielded comparable or better F1 scores than raw full-text chunks. This indicates we can use an LLM to generate a concise version of each endpoint or schema that captures the key info (operations, field names, etc.) before embedding. This lowers token count per chunk and can improve relevance matching by removing noise. For instance, instead of embedding a 500-line schema definition verbatim, we might embed a summary like “Schema Campaign: includes fields id (string), name (string), budget (number), targeting (TargetingCriteria object with fields X, Y, Z), …”.

* **Overlap for Hierarchy:** If you break a long schema into two chunks (say, one chunk for the first 10 properties, another for the rest), include a small overlap (maybe 1–2 properties repeating) or a note in the second chunk like “(Continuation of Campaign schema properties…)”. Overlap helps maintain context so that if a query is about a property near the boundary, at least one chunk will have it in full context. This echoes the general best practice of overlapping segments to preserve context continuity.

* **Embedding Model Constraints:** You mentioned using the Qwen-3 (0.6B) model for embedding. Ensure that your chunk text encoding stays within its input limit (\~8K tokens). Qwen is a relatively smaller model; smaller embedding models might have lower capacity to capture very long context relationships, so feeding extremely large chunks might reduce the quality of embeddings. Therefore, lean towards **smaller, coherent chunks** rather than maxing out the token limit. If needed, test a few chunk sizes (e.g. 512 tokens vs 1024 vs 2048) to see how it affects retrieval accuracy. The optimal size might be where each chunk covers a complete concept but not much more.

* **Consistent Formatting:** Maintain a consistent format for similar chunks. For example, always list schema fields in a bullet list or table-like format in the text. Always start chunks with a clear title line. This consistency helps the model’s embeddings align similar information and can improve semantic search. It also ensures that when the LLM answers, it presents the information in a coherent way (consistency is part of Quality Req. “Consistency” for user experience).

By optimizing chunk content and retrieval in this way, we aim to satisfy the success metrics:

* **Query Coverage:** By inlining refs or linking them, any query that hits an endpoint should also get >95% of related info. If our chunking + retrieval is effective, we’d measure high recall. (The research benchmark RestBench saw recall improving from \~66% with naive splitting to \~75-97% with smarter chunking, depending on strategy. We can target the high end of that by combining strategies.)

* **Precision:** Through logical chunking and trimming irrelevant parts, we avoid huge irrelevant matches. Ideally, the top 3-5 retrieved chunks should be highly relevant to the query. We can manually evaluate a set of test queries (like the scenarios provided) to ensure irrelevant chunks aren’t creeping in. Using metadata filters (like by tag or service if the query clearly implies one) can further boost precision by not surfacing chunks from unrelated APIs.

* **Schema Completion:** We expect that if a user asks about an endpoint or schema, the answer will include all necessary fields and definitions. We can verify this by testing scenarios (below) – for example, ensure that asking for a “complete request example” returns the endpoint and the schema with all required fields, not just half of them.

* **Performance and Maintainability:** Because each API spec is chunked independently and linked via metadata (rather than merged into one giant graph that must be entirely regenerated on each change), updates are localized. If one service’s spec changes, you can re-process only that spec’s chunks. The vector index update can replace just those chunks. This keeps indexing time reasonable even with 100+ specs (Functional Req. #5). Also, avoiding excessive duplication of large schemas across many chunks saves on storage and embedding computations (Performance Req. “Embedding Efficiency” and “Storage Efficiency”). For example, if a `User` schema is reused in 15 endpoints, you might choose not to inline its full definition in all 15 places. Instead, embed it once as a schema chunk and perhaps just reference it by name (or include a brief summary) in the 15 endpoint chunks. This way you only embed the full content once, saving space. The trade-off is a slightly more complex retrieval step – but as discussed, that can be automated.

## Implementation Suggestions

Putting this into practice:

1. **Parse and Build a Reference Graph:** Use an OpenAPI parser (there are libraries in Python, Node, etc. that can load the spec JSON/YAML). Traverse each path and its methods. For each operation, gather its direct schema references (requestBody, responses, parameters schemas, etc.). Also traverse component schemas to find nested `$ref`s. Build a graph or map of relationships: e.g. Operation -> Schema A -> Schema B -> ... etc. This will let you decide what to include where.

2. **Chunk Construction Logic:** Implement a function that given an operation (endpoint) will produce one or more chunk texts:

   * Start with the endpoint’s own info (method, path, summary, description, parameters with their types/descr, etc.).
   * For each `$ref` in that operation (direct references), retrieve the referenced schema definition from the components and inline its definition *in brief*. You might format it as a mini-schema section in the chunk (e.g. “**Request Body Schema – CreateCampaignRequest:** (object) properties: `campaign` (Campaign, see below), `startDate` (string, format date)… Required: campaign, startDate.”). If that referenced schema (CreateCampaignRequest) itself has refs (like `campaign` property references `Campaign` schema), you have a choice: either inline that as well right here (indent it or note it as a sub-section), or if it’s very large, you might just mention it and rely on a separate chunk. For completeness, leaning toward inlining at least one level is good. You could even inline two levels deep in a hierarchical format (sub-bullets for nested properties).
   * The chunk text might thus contain the endpoint description, followed by a **“Schema Details”** section integrating those schemas. Keep an eye on length – if it’s growing too much, you may stop at one level and handle deeper ones separately.

3. **Schema Chunks:** For each top-level schema in components (especially those that are referenced by multiple endpoints or likely to be asked about on their own), create a chunk with its full definition (all properties, types, descriptions, and perhaps example values if brief). Also list if it has sub-schemas (you can choose to inline one level of sub-schema here too, or split them if they’re complex). For very large schemas, splitting into two chunks as mentioned might be necessary – maybe one chunk for an overview and required fields, and another for optional or advanced fields, etc.

4. **Metadata Assignment:** When adding these chunks to your vector store, include metadata:

   * For endpoint chunk: `{"type": "endpoint", "path": "/campaigns", "method": "POST", "operationId": "createCampaign", "refs": ["CreateCampaignRequest", "Campaign", "TargetingCriteria"], "service": "AdCatalog", "tag": "Campaigns"}` (for example).
   * For schema chunk: `{"type": "schema", "name": "Campaign", "used_in": ["CreateCampaignRequest","UpdateCampaignRequest"], "service": "AdCatalog", "tag": "Campaigns"}`.
   * These are just examples; tailor it to what your query logic will use. The point is to make relationships queryable. For instance, if you use Weaviate or Pinecone, you can do filtered vector search like `filter={"name": "Campaign"}` to directly fetch that chunk if you know the name. Or use hybrid search (keyword + vector) to find exact schema names quickly.

5. **Embedding and Indexing:** Embed all chunks with Qwen-3 (ensure text is in a consistent language if your model is language-specific, e.g., all English). Given Qwen’s size (600M), you might want to test its embedding quality; if it struggles, consider a stronger embedding model like text-embedding-ada-002 or similar, since accuracy of semantic search is crucial. However, Qwen’s 8k token context is helpful if your chunks are large. Index these embeddings in your vector DB.

6. **Retrieval Pipeline:** Implement a custom retriever that wraps the raw vector search. This retriever, on a query, will do:

   * Initial vector search to get top-k chunks (say k=5).
   * Look at those results: if any chunk is an endpoint and has `refs` in metadata, automatically do a lookup (could be a metadata filtered vector search or a direct map in memory) for those schema names and append them to the results. If any chunk is a schema that has a `used_in` link relevant to the query context, you might consider also fetching the endpoint for context – though usually user queries drive which endpoint to fetch, so primarily we bring schemas to endpoints rather than endpoints to schemas.
   * Merge and return the expanded set of chunks. You might want to limit the final count (maybe up to 5–6 total) to avoid overloading the LLM. Usually the endpoint + a couple schemas are enough. If more than that come in, you could prioritize by relevance score or some heuristic (e.g., if an endpoint references 5 schemas but the query likely only cares about 2 of them, perhaps omit the ones not directly asked about – this is hard, but a clue is if the query mentions a specific field or sub-object, focus on that).

7. **Testing Scenarios:** Use the provided test scenarios to validate:

   * *Endpoint Discovery:* Ask “How do I create a new sponsored product campaign?” – see if your retriever returns the create campaign endpoint chunk and its schemas. The answer should include the endpoint method/URL and the required fields from the request schema (campaign name, budget, targeting criteria, etc.). If something is missing, adjust chunk content or linking.
   * *Schema Inspection:* Ask “What fields are required in UpdateAttributesRequestContent?” – verify that the chunk for that schema is retrieved and it indeed lists the required fields with descriptions. If your index didn’t have that schema because it wasn’t referenced by an endpoint, you might have to ensure all schemas (even those only mentioned in schemas) are indexed. Often, some component schemas might only be used within other schemas (not directly by an endpoint) – you should still chunk and index them so they can be found.
   * *Error Handling:* “What causes ACCESS\_DENIED error in AdCatalog API?” – this should bring up either an error codes chunk for AdCatalog or any endpoint that mentions ACCESS\_DENIED. If your metadata tags by service, the query likely will match those chunks. Ensure that error codes chunk includes the meaning of ACCESS\_DENIED and perhaps which endpoints or conditions trigger it (e.g. “if the user’s auth token is invalid or they lack permissions”). If not, you might consider adding a brief note in each endpoint chunk that can return that error, like in the responses description.
   * *Cross-API Query:* “Which APIs support campaign frequency capping?” – this is a broader search question. It might match multiple chunks across different services (anywhere “frequency cap” is mentioned). With good chunking, those keywords will appear in the relevant schema or endpoint text. The vector search can retrieve several endpoint chunks from different specs that all mention frequency capping. To make the answer useful, each chunk’s context (via the heading or metadata in content) will tell the LLM which API or service it’s from. The user should see, for example: “AdCatalog API – Campaign settings – field `frequencyCap` (integer, optional)…”, and “AdReporting API – Insights – field `frequencyCap`…”, etc., allowing them to identify all relevant APIs. Consistency in how you label chunks with their API name will help the LLM present the results clearly.

8. **Iterate on Quality:** If you find the LLM’s answers are inconsistent or incomplete, consider refining chunk contents (maybe the wording, or adding an extra sentence of context). For example, if answers are missing the connection that a schema is part of a certain endpoint, maybe include a line in the schema chunk: “**Usage:** Used in Create Campaign and Update Campaign endpoints.” This provides explicit context to the model when formulating an answer. Always preserve crucial relationships either in the text or via retrieval so the model doesn’t have to infer too much.

By following these strategies – intelligent chunk grouping, reference inlining, metadata linking, and careful retrieval logic – your RAG system should retrieve **complete and precise information** for OpenAPI documentation queries. This approach ensures that users get the benefit of the full spec’s knowledge (even deeply nested schemas) in a single answer, meeting the high bar for completeness and accuracy required in enterprise API docs search.
