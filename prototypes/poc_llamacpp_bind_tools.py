"""
Proper LLaMA.cpp bind_tools() Test

Based on research:
1. Use Llama.from_pretrained() to download model first
2. Get the downloaded model path from HF cache
3. Initialize ChatLlamaCpp with proper model_path
4. Test bind_tools() with forced tool_choice
5. Verify actual tool calling functionality
"""

import asyncio
import sys
from pathlib import Path
from typing import Optional

# LangChain and LangGraph imports
from langchain_core.tools import tool
from langgraph.prebuilt import create_react_agent

@tool
def simple_math_tool(a: int, b: int) -> int:
    """Add two numbers together. Use this when you need to do math."""
    print(f"ğŸ§® Math tool called: {a} + {b} = {a + b}")
    return a + b

@tool  
def weather_tool(city: str) -> str:
    """Get weather for a city. Use this when asked about weather."""
    print(f"ğŸŒ¤ï¸ Weather tool called for: {city}")
    return f"The weather in {city} is sunny and 75Â°F"

def download_model_and_get_path():
    """Download model from HuggingFace and return the local path"""
    try:
        from llama_cpp import Llama
        import os
        
        print("ğŸ“¥ Downloading Qwen3 4B model from HuggingFace...")
        
        # Download model using from_pretrained - this caches it locally
        # Using Qwen3 4B which supports tool calling
        base_llm = Llama.from_pretrained(
            repo_id="Qwen/Qwen3-4B-GGUF",
            filename="*Q4_K_M.gguf", 
            n_ctx=2048,
            verbose=False
        )
        
        print("âœ… Model downloaded and loaded")
        
        # Get the model path from the loaded model
        # The model should have a model_path attribute or similar
        if hasattr(base_llm, 'model_path'):
            model_path = base_llm.model_path
            print(f"ğŸ“ Model path: {model_path}")
            return model_path
        else:
            # Try to find it in HF cache
            from huggingface_hub import snapshot_download
            
            cache_dir = snapshot_download(
                repo_id="Qwen/Qwen3-4B-GGUF",
                allow_patterns="*Q4_K_M.gguf"
            )
            
            # Find the .gguf file
            gguf_files = list(Path(cache_dir).glob("*.gguf"))
            if gguf_files:
                model_path = str(gguf_files[0])
                print(f"ğŸ“ Found model at: {model_path}")
                return model_path
            else:
                raise FileNotFoundError("Could not find downloaded GGUF file")
        
    except Exception as e:
        print(f"âŒ Model download failed: {e}")
        return None

def test_llamacpp_bind_tools_proper():
    """Test ChatLlamaCpp bind_tools() with proper initialization"""
    print("\nğŸ§ª Test 1: ChatLlamaCpp with Proper bind_tools()")
    print("-" * 60)
    
    try:
        from langchain_community.chat_models import ChatLlamaCpp
        
        # Get the proper model path
        model_path = download_model_and_get_path()
        if model_path is None:
            return False
        
        # Initialize ChatLlamaCpp with the correct model path
        model = ChatLlamaCpp(
            model_path=model_path,
            temperature=0.1,
            max_tokens=200,
            n_ctx=2048,
            verbose=False
        )
        
        print("âœ… ChatLlamaCpp initialized successfully")
        
        # Test basic functionality first
        response = model.invoke("What is 2+2? Just give the number.")
        print(f"ğŸ“ Basic test: {response.content}")
        
        # Now test bind_tools
        tools = [simple_math_tool, weather_tool]
        model_with_tools = model.bind_tools(tools)
        
        print("âœ… Tools bound successfully")
        
        # Test with forced tool choice (as per research)
        tool_choice = {
            "type": "function", 
            "function": {"name": "simple_math_tool"}
        }
        
        print("ğŸ”§ Testing with forced tool choice...")
        
        # Invoke with tool choice
        response = model_with_tools.invoke(
            "What is 15 + 27?",
            tool_choice=tool_choice
        )
        
        print(f"ğŸ“ Response type: {type(response)}")
        print(f"ğŸ“ Response content: {response.content}")
        
        # Check for tool calls
        if hasattr(response, 'tool_calls') and response.tool_calls:
            print(f"ğŸ”§ Tool calls found: {len(response.tool_calls)}")
            for tool_call in response.tool_calls:
                print(f"   - Tool: {tool_call}")
                
                # Actually execute the tool
                if tool_call.get('name') == 'simple_math_tool':
                    args = tool_call.get('args', {})
                    result = simple_math_tool.invoke(args)
                    print(f"   - Result: {result}")
            
            return True
        else:
            print("âŒ No tool calls found")
            
            # Try different approach - check response for tool usage patterns
            if 'function' in str(response.additional_kwargs):
                print("ğŸ”§ Tool usage found in additional_kwargs")
                print(f"   - Additional kwargs: {response.additional_kwargs}")
                return True
            
            return False
            
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        return False

def test_llamacpp_multiple_tools():
    """Test ChatLlamaCpp with multiple tools and auto-selection limitation"""
    print("\nğŸ§ª Test 2: Multiple Tools (Expected to Need Manual Choice)")
    print("-" * 60)
    
    try:
        from langchain_community.chat_models import ChatLlamaCpp
        
        model_path = download_model_and_get_path()
        if model_path is None:
            return False
        
        model = ChatLlamaCpp(
            model_path=model_path,
            temperature=0.1,
            max_tokens=200,
            n_ctx=2048,
            verbose=False
        )
        
        tools = [simple_math_tool, weather_tool]
        model_with_tools = model.bind_tools(tools)
        
        print("âœ… Multiple tools bound")
        
        # Try without tool_choice (should fail auto-selection)
        try:
            response = model_with_tools.invoke("What is 10 + 5 and what's the weather in Paris?")
            print(f"ğŸ“ Auto-selection response: {response.content}")
            
            if hasattr(response, 'tool_calls') and response.tool_calls:
                print("ğŸ‰ Unexpected success - auto tool selection worked!")
                return True
            else:
                print("âŒ No tool calls - auto-selection limitation confirmed")
                
                # Now try with specific tool choice
                tool_choice = {"type": "function", "function": {"name": "simple_math_tool"}}
                response2 = model_with_tools.invoke("What is 10 + 5?", tool_choice=tool_choice)
                
                if hasattr(response2, 'tool_calls') and response2.tool_calls:
                    print("âœ… Forced tool choice works")
                    return True
                else:
                    print("âŒ Even forced tool choice failed")
                    return False
                    
        except Exception as e:
            print(f"âŒ Multiple tools test failed: {e}")
            return False
            
    except Exception as e:
        print(f"âŒ Test setup failed: {e}")
        return False

async def test_llamacpp_react_agent_with_proper_model():
    """Test if create_react_agent works with properly initialized ChatLlamaCpp"""
    print("\nğŸ§ª Test 3: create_react_agent with Proper Model")
    print("-" * 60)
    
    try:
        from langchain_community.chat_models import ChatLlamaCpp
        
        model_path = download_model_and_get_path()
        if model_path is None:
            return False
        
        model = ChatLlamaCpp(
            model_path=model_path,
            temperature=0.1,
            max_tokens=200,
            n_ctx=2048,
            verbose=False
        )
        
        # Try create_react_agent
        agent = create_react_agent(
            model=model,
            tools=[simple_math_tool],
            prompt="You are a helpful assistant. Use tools when appropriate."
        )
        
        print("âœ… ReAct agent created successfully!")
        
        # Test the agent
        result = await agent.ainvoke({
            "messages": [{"role": "user", "content": "What is 12 + 8?"}]
        })
        
        print("âœ… Agent responded")
        print(f"ğŸ“ Message count: {len(result['messages'])}")
        
        # Check for tool usage in messages
        tool_used = False
        for msg in result['messages']:
            if hasattr(msg, 'tool_calls') and msg.tool_calls:
                print(f"ğŸ”§ Tool calls found in message: {msg.tool_calls}")
                tool_used = True
        
        if tool_used:
            print("ğŸ‰ ReAct agent successfully used tools!")
            return True
        else:
            print("âŒ ReAct agent didn't use tools (limitation confirmed)")
            return False
            
    except Exception as e:
        print(f"âŒ ReAct agent test failed: {e}")
        return False

def test_environment():
    """Check if environment is ready"""
    print("ğŸ”§ Environment Check:")
    print(f"  - Python version: {sys.version.split()[0]}")
    
    try:
        from langchain_community.chat_models import ChatLlamaCpp
        from langgraph.prebuilt import create_react_agent
        from llama_cpp import Llama
        from huggingface_hub import snapshot_download
        print("  - Required packages: âœ… Available")
        return True
    except ImportError as e:
        print(f"  - Required packages: âŒ Missing ({e})")
        return False

async def main():
    """Run proper ChatLlamaCpp bind_tools tests"""
    print("ğŸš€ ChatLlamaCpp bind_tools() Proof of Concept (Proper)")
    print("=" * 60)
    
    if not test_environment():
        return False
    
    results = []
    
    # Test 1: Proper bind_tools with forced tool choice
    results.append(test_llamacpp_bind_tools_proper())
    
    # Test 2: Multiple tools limitation
    results.append(test_llamacpp_multiple_tools())
    
    # Test 3: ReAct agent compatibility
    results.append(await test_llamacpp_react_agent_with_proper_model())
    
    # Summary
    print("\nğŸ“Š Test Results Summary")
    print("-" * 30)
    passed = sum(results)
    total = len(results)
    print(f"âœ… Passed: {passed}/{total}")
    
    if results[0]:
        print("âœ… ChatLlamaCpp bind_tools() works with forced tool choice")
    if results[1]:
        print("âœ… Multiple tools work but need manual selection")
    if results[2]:
        print("âœ… ReAct agent compatible (unexpected!)")
    else:
        print("âŒ ReAct agent incompatible (expected due to auto-selection)")
    
    if passed >= 1:
        print("\nğŸ‰ SUCCESS! ChatLlamaCpp bind_tools() is functional!")
        print("ğŸ’¡ Key finding: Manual tool choice required, no auto-selection")
        return True
    else:
        print("\nâŒ FAILED! ChatLlamaCpp bind_tools() not working")
        return False

if __name__ == "__main__":
    asyncio.run(main())